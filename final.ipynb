{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ritesh\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\distributions\\distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\Ritesh\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\distributions\\bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "Epoch 1/200\n",
      "188/188 [==============================] - 9s 23ms/step - loss: 15.4953 - mae: 15.4953 - val_loss: 11.9690 - val_mae: 11.9690\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 4s 19ms/step - loss: 10.8389 - mae: 10.8389 - val_loss: 9.9238 - val_mae: 9.9238\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.2953 - mae: 10.2953 - val_loss: 10.0734 - val_mae: 10.0734\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.2557 - mae: 10.2557 - val_loss: 9.9520 - val_mae: 9.9520\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.2149 - mae: 10.2149 - val_loss: 9.8379 - val_mae: 9.8379\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.2110 - mae: 10.2110 - val_loss: 9.8917 - val_mae: 9.8917\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1902 - mae: 10.1902 - val_loss: 9.8839 - val_mae: 9.8839\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1986 - mae: 10.1986 - val_loss: 9.9104 - val_mae: 9.9104\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1766 - mae: 10.1766 - val_loss: 9.9192 - val_mae: 9.9192\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1767 - mae: 10.1767 - val_loss: 9.8615 - val_mae: 9.8615\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1338 - mae: 10.1338 - val_loss: 9.8568 - val_mae: 9.8568\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1946 - mae: 10.1946 - val_loss: 9.8473 - val_mae: 9.8473\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1804 - mae: 10.1804 - val_loss: 9.8472 - val_mae: 9.8472\n",
      "Epoch 14/200\n",
      "188/188 [==============================] - 4s 19ms/step - loss: 10.1594 - mae: 10.1594 - val_loss: 9.9041 - val_mae: 9.9041\n",
      "Epoch 15/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1779 - mae: 10.1779 - val_loss: 9.8400 - val_mae: 9.8400\n",
      "Epoch 16/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1706 - mae: 10.1706 - val_loss: 9.8605 - val_mae: 9.8605\n",
      "Epoch 17/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1335 - mae: 10.1335 - val_loss: 9.8802 - val_mae: 9.8802\n",
      "Epoch 18/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1506 - mae: 10.1506 - val_loss: 9.8355 - val_mae: 9.8355\n",
      "Epoch 19/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1138 - mae: 10.1138 - val_loss: 9.8714 - val_mae: 9.8714\n",
      "Epoch 20/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1412 - mae: 10.1412 - val_loss: 9.8589 - val_mae: 9.8589\n",
      "Epoch 21/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1332 - mae: 10.1332 - val_loss: 9.8233 - val_mae: 9.8233\n",
      "Epoch 22/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1452 - mae: 10.1452 - val_loss: 9.9062 - val_mae: 9.9062\n",
      "Epoch 23/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1501 - mae: 10.1501 - val_loss: 9.8453 - val_mae: 9.8453\n",
      "Epoch 24/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1565 - mae: 10.1565 - val_loss: 9.9423 - val_mae: 9.9423\n",
      "Epoch 25/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1232 - mae: 10.1232 - val_loss: 9.8481 - val_mae: 9.8481\n",
      "Epoch 26/200\n",
      "188/188 [==============================] - 3s 15ms/step - loss: 10.1518 - mae: 10.1518 - val_loss: 9.8374 - val_mae: 9.8374\n",
      "Epoch 27/200\n",
      "188/188 [==============================] - 3s 15ms/step - loss: 10.1583 - mae: 10.1583 - val_loss: 9.8557 - val_mae: 9.8557\n",
      "Epoch 28/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1697 - mae: 10.1697 - val_loss: 9.8445 - val_mae: 9.8445\n",
      "Epoch 29/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1348 - mae: 10.1348 - val_loss: 9.9054 - val_mae: 9.9054\n",
      "Epoch 30/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1409 - mae: 10.1409 - val_loss: 9.8449 - val_mae: 9.8449\n",
      "Epoch 31/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1328 - mae: 10.1328 - val_loss: 9.8562 - val_mae: 9.8562\n",
      "Epoch 32/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1356 - mae: 10.1356 - val_loss: 9.8463 - val_mae: 9.8463\n",
      "Epoch 33/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1197 - mae: 10.1197 - val_loss: 9.8759 - val_mae: 9.8759\n",
      "Epoch 34/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1208 - mae: 10.1208 - val_loss: 9.8655 - val_mae: 9.8655\n",
      "Epoch 35/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1136 - mae: 10.1136 - val_loss: 9.8398 - val_mae: 9.8398\n",
      "Epoch 36/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1351 - mae: 10.1351 - val_loss: 9.8367 - val_mae: 9.8367\n",
      "Epoch 37/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1232 - mae: 10.1232 - val_loss: 9.8890 - val_mae: 9.8890\n",
      "Epoch 38/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1318 - mae: 10.1318 - val_loss: 9.8420 - val_mae: 9.8420\n",
      "Epoch 39/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1148 - mae: 10.1148 - val_loss: 9.8434 - val_mae: 9.8434\n",
      "Epoch 40/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1076 - mae: 10.1076 - val_loss: 9.8776 - val_mae: 9.8776\n",
      "Epoch 41/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1342 - mae: 10.1342 - val_loss: 9.8657 - val_mae: 9.8657\n",
      "Epoch 42/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1191 - mae: 10.1191 - val_loss: 9.8686 - val_mae: 9.8686\n",
      "Epoch 43/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1267 - mae: 10.1267 - val_loss: 9.8371 - val_mae: 9.8371\n",
      "Epoch 44/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1150 - mae: 10.1150 - val_loss: 9.8343 - val_mae: 9.8343\n",
      "Epoch 45/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1216 - mae: 10.1216 - val_loss: 9.8363 - val_mae: 9.8363\n",
      "Epoch 46/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1207 - mae: 10.1207 - val_loss: 9.8674 - val_mae: 9.8674\n",
      "Epoch 47/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1562 - mae: 10.1562 - val_loss: 9.8520 - val_mae: 9.8520\n",
      "Epoch 48/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.0882 - mae: 10.0882 - val_loss: 9.8560 - val_mae: 9.8560\n",
      "Epoch 49/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1215 - mae: 10.1215 - val_loss: 9.8455 - val_mae: 9.8455\n",
      "Epoch 50/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1193 - mae: 10.1193 - val_loss: 9.8394 - val_mae: 9.8394\n",
      "Epoch 51/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1234 - mae: 10.1234 - val_loss: 9.8526 - val_mae: 9.8526\n",
      "Epoch 52/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1204 - mae: 10.1204 - val_loss: 9.8372 - val_mae: 9.8372\n",
      "Epoch 53/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1316 - mae: 10.1316 - val_loss: 9.8437 - val_mae: 9.8437\n",
      "Epoch 54/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1228 - mae: 10.1228 - val_loss: 9.8607 - val_mae: 9.8607\n",
      "Epoch 55/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0905 - mae: 10.0905 - val_loss: 9.8252 - val_mae: 9.8252\n",
      "Epoch 56/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1132 - mae: 10.1132 - val_loss: 9.8312 - val_mae: 9.8312\n",
      "Epoch 57/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1285 - mae: 10.1285 - val_loss: 9.8328 - val_mae: 9.8328\n",
      "Epoch 58/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1262 - mae: 10.1262 - val_loss: 9.8416 - val_mae: 9.8416\n",
      "Epoch 59/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.0963 - mae: 10.0963 - val_loss: 9.8412 - val_mae: 9.8412\n",
      "Epoch 60/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1088 - mae: 10.1088 - val_loss: 9.8522 - val_mae: 9.8522\n",
      "Epoch 61/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1144 - mae: 10.1144 - val_loss: 9.8316 - val_mae: 9.8316\n",
      "Epoch 62/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1191 - mae: 10.1191 - val_loss: 9.8659 - val_mae: 9.8659\n",
      "Epoch 63/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1168 - mae: 10.1168 - val_loss: 9.8254 - val_mae: 9.8254\n",
      "Epoch 64/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1152 - mae: 10.1152 - val_loss: 9.8575 - val_mae: 9.8575\n",
      "Epoch 65/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1149 - mae: 10.1149 - val_loss: 9.9022 - val_mae: 9.9022\n",
      "Epoch 66/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.1163 - mae: 10.1163 - val_loss: 9.8885 - val_mae: 9.8885\n",
      "Epoch 67/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.0840 - mae: 10.0840 - val_loss: 9.8352 - val_mae: 9.8352\n",
      "Epoch 68/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1130 - mae: 10.1130 - val_loss: 9.8270 - val_mae: 9.8270\n",
      "Epoch 69/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1222 - mae: 10.1222 - val_loss: 9.8497 - val_mae: 9.8497\n",
      "Epoch 70/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.0852 - mae: 10.0852 - val_loss: 9.8489 - val_mae: 9.8489\n",
      "Epoch 71/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.0990 - mae: 10.0990 - val_loss: 9.8651 - val_mae: 9.8651\n",
      "Epoch 72/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.0752 - mae: 10.0752 - val_loss: 9.9422 - val_mae: 9.9422\n",
      "Epoch 73/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1009 - mae: 10.1009 - val_loss: 9.8418 - val_mae: 9.8418\n",
      "Epoch 74/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1049 - mae: 10.1049 - val_loss: 9.8384 - val_mae: 9.8384\n",
      "Epoch 75/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0830 - mae: 10.0830 - val_loss: 9.8196 - val_mae: 9.8196\n",
      "Epoch 76/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1086 - mae: 10.1086 - val_loss: 9.8261 - val_mae: 9.8261\n",
      "Epoch 77/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1214 - mae: 10.1214 - val_loss: 9.8406 - val_mae: 9.8406\n",
      "Epoch 78/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1082 - mae: 10.1082 - val_loss: 9.8256 - val_mae: 9.8256\n",
      "Epoch 79/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0903 - mae: 10.0903 - val_loss: 9.8373 - val_mae: 9.8373\n",
      "Epoch 80/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1070 - mae: 10.1070 - val_loss: 9.8232 - val_mae: 9.8232\n",
      "Epoch 81/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1055 - mae: 10.1055 - val_loss: 9.8870 - val_mae: 9.8870\n",
      "Epoch 82/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1094 - mae: 10.1094 - val_loss: 9.8601 - val_mae: 9.8601\n",
      "Epoch 83/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1247 - mae: 10.1247 - val_loss: 9.8236 - val_mae: 9.8236\n",
      "Epoch 84/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0874 - mae: 10.0874 - val_loss: 9.8411 - val_mae: 9.8411\n",
      "Epoch 85/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0902 - mae: 10.0902 - val_loss: 9.8359 - val_mae: 9.8359\n",
      "Epoch 86/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0989 - mae: 10.0989 - val_loss: 9.8496 - val_mae: 9.8496\n",
      "Epoch 87/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0998 - mae: 10.0998 - val_loss: 9.8527 - val_mae: 9.8527\n",
      "Epoch 88/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1002 - mae: 10.1002 - val_loss: 9.8252 - val_mae: 9.8252\n",
      "Epoch 89/200\n",
      "188/188 [==============================] - 3s 18ms/step - loss: 10.0886 - mae: 10.0886 - val_loss: 9.8586 - val_mae: 9.8586\n",
      "Epoch 90/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1181 - mae: 10.1181 - val_loss: 9.8282 - val_mae: 9.8282\n",
      "Epoch 91/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1146 - mae: 10.1146 - val_loss: 9.8210 - val_mae: 9.8210\n",
      "Epoch 92/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1079 - mae: 10.1079 - val_loss: 9.8308 - val_mae: 9.8308\n",
      "Epoch 93/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1214 - mae: 10.1214 - val_loss: 9.8428 - val_mae: 9.8428\n",
      "Epoch 94/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0973 - mae: 10.0973 - val_loss: 9.8391 - val_mae: 9.8391\n",
      "Epoch 95/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0789 - mae: 10.0789 - val_loss: 9.8238 - val_mae: 9.8238\n",
      "Epoch 96/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0934 - mae: 10.0934 - val_loss: 9.8412 - val_mae: 9.8412\n",
      "Epoch 97/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0760 - mae: 10.0760 - val_loss: 9.8134 - val_mae: 9.8134\n",
      "Epoch 98/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0631 - mae: 10.0631 - val_loss: 9.8128 - val_mae: 9.8128\n",
      "Epoch 99/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1040 - mae: 10.1040 - val_loss: 9.8579 - val_mae: 9.8579\n",
      "Epoch 100/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1092 - mae: 10.1092 - val_loss: 9.8194 - val_mae: 9.8194\n",
      "Epoch 101/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1001 - mae: 10.1001 - val_loss: 9.8337 - val_mae: 9.8337\n",
      "Epoch 102/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0795 - mae: 10.0795 - val_loss: 9.8233 - val_mae: 9.8233\n",
      "Epoch 103/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0914 - mae: 10.0914 - val_loss: 9.8255 - val_mae: 9.8255\n",
      "Epoch 104/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1045 - mae: 10.1045 - val_loss: 9.8317 - val_mae: 9.8317\n",
      "Epoch 105/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1053 - mae: 10.1053 - val_loss: 9.8256 - val_mae: 9.8256\n",
      "Epoch 106/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0972 - mae: 10.0972 - val_loss: 9.8488 - val_mae: 9.8488\n",
      "Epoch 107/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0856 - mae: 10.0856 - val_loss: 9.8340 - val_mae: 9.8340\n",
      "Epoch 108/200\n",
      "188/188 [==============================] - 3s 15ms/step - loss: 10.0829 - mae: 10.0829 - val_loss: 9.8324 - val_mae: 9.8324\n",
      "Epoch 109/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1049 - mae: 10.1049 - val_loss: 9.8225 - val_mae: 9.8225\n",
      "Epoch 110/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0881 - mae: 10.0881 - val_loss: 9.8176 - val_mae: 9.8176\n",
      "Epoch 111/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0801 - mae: 10.0801 - val_loss: 9.8281 - val_mae: 9.8281\n",
      "Epoch 112/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0929 - mae: 10.0929 - val_loss: 9.8289 - val_mae: 9.8289\n",
      "Epoch 113/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0873 - mae: 10.0873 - val_loss: 9.8435 - val_mae: 9.8435\n",
      "Epoch 114/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1057 - mae: 10.1057 - val_loss: 9.8238 - val_mae: 9.8238\n",
      "Epoch 115/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0773 - mae: 10.0773 - val_loss: 9.8395 - val_mae: 9.8395\n",
      "Epoch 116/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0967 - mae: 10.0967 - val_loss: 9.8424 - val_mae: 9.8424\n",
      "Epoch 117/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0722 - mae: 10.0722 - val_loss: 9.8138 - val_mae: 9.8138\n",
      "Epoch 118/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1083 - mae: 10.1083 - val_loss: 9.8364 - val_mae: 9.8364\n",
      "Epoch 119/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0908 - mae: 10.0908 - val_loss: 9.8635 - val_mae: 9.8635\n",
      "Epoch 120/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.0860 - mae: 10.0860 - val_loss: 9.8544 - val_mae: 9.8544\n",
      "Epoch 121/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1108 - mae: 10.1108 - val_loss: 9.8519 - val_mae: 9.8519\n",
      "Epoch 122/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0719 - mae: 10.0719 - val_loss: 9.8275 - val_mae: 9.8275\n",
      "Epoch 123/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1062 - mae: 10.1062 - val_loss: 9.8198 - val_mae: 9.8198\n",
      "Epoch 124/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1055 - mae: 10.1055 - val_loss: 9.8243 - val_mae: 9.8243\n",
      "Epoch 125/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0754 - mae: 10.0754 - val_loss: 9.8405 - val_mae: 9.8405\n",
      "Epoch 126/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1126 - mae: 10.1126 - val_loss: 9.8279 - val_mae: 9.8279\n",
      "Epoch 127/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0865 - mae: 10.0865 - val_loss: 9.8458 - val_mae: 9.8458\n",
      "Epoch 128/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0864 - mae: 10.0864 - val_loss: 9.8331 - val_mae: 9.8331\n",
      "Epoch 129/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1035 - mae: 10.1035 - val_loss: 9.8223 - val_mae: 9.8223\n",
      "Epoch 130/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0973 - mae: 10.0973 - val_loss: 9.8292 - val_mae: 9.8292\n",
      "Epoch 131/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1114 - mae: 10.1114 - val_loss: 9.8519 - val_mae: 9.8519\n",
      "Epoch 132/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0738 - mae: 10.0738 - val_loss: 9.8297 - val_mae: 9.8297\n",
      "Epoch 133/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.0883 - mae: 10.0883 - val_loss: 9.8223 - val_mae: 9.8223\n",
      "Epoch 134/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1028 - mae: 10.1028 - val_loss: 9.8344 - val_mae: 9.8344\n",
      "Epoch 135/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0843 - mae: 10.0843 - val_loss: 9.8331 - val_mae: 9.8331\n",
      "Epoch 136/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0961 - mae: 10.0961 - val_loss: 9.8879 - val_mae: 9.8879\n",
      "Epoch 137/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.0762 - mae: 10.0762 - val_loss: 9.8354 - val_mae: 9.8354\n",
      "Epoch 138/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0674 - mae: 10.0674 - val_loss: 9.8410 - val_mae: 9.8410\n",
      "Epoch 139/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.0900 - mae: 10.0900 - val_loss: 9.8331 - val_mae: 9.8331\n",
      "Epoch 140/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0634 - mae: 10.0634 - val_loss: 9.8678 - val_mae: 9.8678\n",
      "Epoch 141/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0953 - mae: 10.0953 - val_loss: 9.8243 - val_mae: 9.8243\n",
      "Epoch 142/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.0937 - mae: 10.0937 - val_loss: 9.8313 - val_mae: 9.8313\n",
      "Epoch 143/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.0696 - mae: 10.0696 - val_loss: 9.8215 - val_mae: 9.8215\n",
      "Epoch 144/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0849 - mae: 10.0849 - val_loss: 9.8220 - val_mae: 9.8220\n",
      "Epoch 145/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0974 - mae: 10.0974 - val_loss: 9.8452 - val_mae: 9.8452\n",
      "Epoch 146/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0708 - mae: 10.0708 - val_loss: 9.8321 - val_mae: 9.8321\n",
      "Epoch 147/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0850 - mae: 10.0850 - val_loss: 9.8306 - val_mae: 9.8306\n",
      "Epoch 148/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0569 - mae: 10.0569 - val_loss: 9.8315 - val_mae: 9.8315\n",
      "Epoch 149/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.1103 - mae: 10.1103 - val_loss: 9.8286 - val_mae: 9.8286\n",
      "Epoch 150/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0944 - mae: 10.0944 - val_loss: 9.8386 - val_mae: 9.8386\n",
      "Epoch 151/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0862 - mae: 10.0862 - val_loss: 9.8636 - val_mae: 9.8636\n",
      "Epoch 152/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0753 - mae: 10.0753 - val_loss: 9.8437 - val_mae: 9.8437\n",
      "Epoch 153/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0865 - mae: 10.0865 - val_loss: 9.8950 - val_mae: 9.8950\n",
      "Epoch 154/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1142 - mae: 10.1142 - val_loss: 9.8381 - val_mae: 9.8381\n",
      "Epoch 155/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0663 - mae: 10.0663 - val_loss: 9.8561 - val_mae: 9.8561\n",
      "Epoch 156/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0995 - mae: 10.0995 - val_loss: 9.8482 - val_mae: 9.8482\n",
      "Epoch 157/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0822 - mae: 10.0822 - val_loss: 9.8771 - val_mae: 9.8771\n",
      "Epoch 158/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0742 - mae: 10.0742 - val_loss: 9.8333 - val_mae: 9.8333\n",
      "Epoch 159/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0898 - mae: 10.0898 - val_loss: 9.8442 - val_mae: 9.8442\n",
      "Epoch 160/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0885 - mae: 10.0885 - val_loss: 9.8264 - val_mae: 9.8264\n",
      "Epoch 161/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0652 - mae: 10.0652 - val_loss: 9.8320 - val_mae: 9.8320\n",
      "Epoch 162/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0954 - mae: 10.0954 - val_loss: 9.8343 - val_mae: 9.8343\n",
      "Epoch 163/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0718 - mae: 10.0718 - val_loss: 9.8549 - val_mae: 9.8549\n",
      "Epoch 164/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1090 - mae: 10.1090 - val_loss: 9.8564 - val_mae: 9.8564\n",
      "Epoch 165/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0801 - mae: 10.0801 - val_loss: 9.8287 - val_mae: 9.8287\n",
      "Epoch 166/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0673 - mae: 10.0673 - val_loss: 9.8414 - val_mae: 9.8414\n",
      "Epoch 167/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0838 - mae: 10.0838 - val_loss: 9.8362 - val_mae: 9.8362\n",
      "Epoch 168/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0674 - mae: 10.0674 - val_loss: 9.8459 - val_mae: 9.8459\n",
      "Epoch 169/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0969 - mae: 10.0969 - val_loss: 9.8287 - val_mae: 9.8287\n",
      "Epoch 170/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0540 - mae: 10.0540 - val_loss: 9.8252 - val_mae: 9.8252\n",
      "Epoch 171/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0875 - mae: 10.0875 - val_loss: 9.8372 - val_mae: 9.8372\n",
      "Epoch 172/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0599 - mae: 10.0599 - val_loss: 9.8339 - val_mae: 9.8339\n",
      "Epoch 173/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0740 - mae: 10.0740 - val_loss: 9.8209 - val_mae: 9.8209\n",
      "Epoch 174/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0750 - mae: 10.0750 - val_loss: 9.8280 - val_mae: 9.8280\n",
      "Epoch 175/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0650 - mae: 10.0650 - val_loss: 9.8383 - val_mae: 9.8383\n",
      "Epoch 176/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0774 - mae: 10.0774 - val_loss: 9.8661 - val_mae: 9.8661\n",
      "Epoch 177/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0877 - mae: 10.0877 - val_loss: 9.8286 - val_mae: 9.8286\n",
      "Epoch 178/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0980 - mae: 10.0980 - val_loss: 9.8289 - val_mae: 9.8289\n",
      "Epoch 179/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0986 - mae: 10.0986 - val_loss: 9.8205 - val_mae: 9.8205\n",
      "Epoch 180/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0709 - mae: 10.0709 - val_loss: 9.8543 - val_mae: 9.8543\n",
      "Epoch 181/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0793 - mae: 10.0793 - val_loss: 9.8291 - val_mae: 9.8291\n",
      "Epoch 182/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0791 - mae: 10.0791 - val_loss: 9.8385 - val_mae: 9.8385\n",
      "Epoch 183/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1002 - mae: 10.1002 - val_loss: 9.8289 - val_mae: 9.8289\n",
      "Epoch 184/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0630 - mae: 10.0630 - val_loss: 9.8315 - val_mae: 9.8315\n",
      "Epoch 185/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0636 - mae: 10.0636 - val_loss: 9.8376 - val_mae: 9.8376\n",
      "Epoch 186/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1105 - mae: 10.1105 - val_loss: 9.8608 - val_mae: 9.8608\n",
      "Epoch 187/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0476 - mae: 10.0476 - val_loss: 9.8569 - val_mae: 9.8569\n",
      "Epoch 188/200\n",
      "188/188 [==============================] - 3s 17ms/step - loss: 10.0864 - mae: 10.0864 - val_loss: 9.8476 - val_mae: 9.8476\n",
      "Epoch 189/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0850 - mae: 10.0850 - val_loss: 9.8692 - val_mae: 9.8692\n",
      "Epoch 190/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0849 - mae: 10.0849 - val_loss: 9.8427 - val_mae: 9.8427\n",
      "Epoch 191/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0800 - mae: 10.0800 - val_loss: 9.8299 - val_mae: 9.8299\n",
      "Epoch 192/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0738 - mae: 10.0738 - val_loss: 9.8231 - val_mae: 9.8231\n",
      "Epoch 193/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1058 - mae: 10.1058 - val_loss: 9.8368 - val_mae: 9.8368\n",
      "Epoch 194/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0919 - mae: 10.0919 - val_loss: 9.8442 - val_mae: 9.8442\n",
      "Epoch 195/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.1005 - mae: 10.1005 - val_loss: 9.8482 - val_mae: 9.8482\n",
      "Epoch 196/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0791 - mae: 10.0791 - val_loss: 9.8321 - val_mae: 9.8321\n",
      "Epoch 197/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0891 - mae: 10.0891 - val_loss: 9.8258 - val_mae: 9.8258\n",
      "Epoch 198/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0797 - mae: 10.0797 - val_loss: 9.8514 - val_mae: 9.8514\n",
      "Epoch 199/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0820 - mae: 10.0820 - val_loss: 9.8386 - val_mae: 9.8386\n",
      "Epoch 200/200\n",
      "188/188 [==============================] - 3s 16ms/step - loss: 10.0567 - mae: 10.0567 - val_loss: 9.8330 - val_mae: 9.8330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dcde235d90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good one\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('NewData.csv')\n",
    "\n",
    "# # Normalize features\n",
    "# data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Split into train and test\n",
    "X = data[['RMT %', 'AGE', 'Depression', 'Stress', 'Anxiety']]\n",
    "y = data['Depression.1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))  \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "            \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "          epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step\n",
      "Predicted Depression: 14.95\n"
     ]
    }
   ],
   "source": [
    "x_in = [[0.32, 68, 38, 18, 8]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Depression: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Depression_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 370ms/step\n",
      "Predicted Depression: 14.95\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Depression_1.h5')\n",
    "\n",
    "x_in = [[0.32, 68, 38, 18, 8]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Depression: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ritesh\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\distributions\\distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\Ritesh\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\distributions\\bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "Epoch 1/200\n",
      "188/188 [==============================] - 6s 12ms/step - loss: 196.7278 - mae: 10.1215 - mse: 196.7278 - val_loss: 163.8799 - val_mae: 9.4363 - val_mse: 163.8799\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 118.2750 - mae: 8.3189 - mse: 118.2750 - val_loss: 105.4713 - val_mae: 8.2501 - val_mse: 105.4713\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 106.9642 - mae: 8.3090 - mse: 106.9642 - val_loss: 102.1640 - val_mae: 8.1919 - val_mse: 102.1640\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 106.4709 - mae: 8.3148 - mse: 106.4709 - val_loss: 101.7921 - val_mae: 8.2127 - val_mse: 101.7921\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 107.1274 - mae: 8.3671 - mse: 107.1274 - val_loss: 101.0642 - val_mae: 8.0996 - val_mse: 101.0642\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.6156 - mae: 8.2735 - mse: 105.6156 - val_loss: 103.1738 - val_mae: 8.1627 - val_mse: 103.1738\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 106.1640 - mae: 8.3075 - mse: 106.1640 - val_loss: 101.6872 - val_mae: 8.1260 - val_mse: 101.6872\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.2776 - mae: 8.2611 - mse: 105.2776 - val_loss: 100.6992 - val_mae: 8.1127 - val_mse: 100.6992\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 105.6720 - mae: 8.2733 - mse: 105.6720 - val_loss: 100.8880 - val_mae: 8.1177 - val_mse: 100.8880\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.3521 - mae: 8.2754 - mse: 105.3521 - val_loss: 100.9601 - val_mae: 8.1559 - val_mse: 100.9601\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.9429 - mae: 8.2493 - mse: 104.9429 - val_loss: 100.7780 - val_mae: 8.1202 - val_mse: 100.7780\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.0227 - mae: 8.2758 - mse: 105.0227 - val_loss: 101.3705 - val_mae: 8.1432 - val_mse: 101.3705\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.4932 - mae: 8.2721 - mse: 105.4932 - val_loss: 100.4617 - val_mae: 8.0909 - val_mse: 100.4617\n",
      "Epoch 14/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.0452 - mae: 8.2771 - mse: 105.0452 - val_loss: 101.5683 - val_mae: 8.1408 - val_mse: 101.5683\n",
      "Epoch 15/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 105.0162 - mae: 8.2708 - mse: 105.0162 - val_loss: 102.1542 - val_mae: 8.1852 - val_mse: 102.1542\n",
      "Epoch 16/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.0355 - mae: 8.2683 - mse: 105.0355 - val_loss: 100.6979 - val_mae: 8.0509 - val_mse: 100.6979\n",
      "Epoch 17/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.0417 - mae: 8.2679 - mse: 105.0417 - val_loss: 100.4493 - val_mae: 8.0600 - val_mse: 100.4493\n",
      "Epoch 18/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.6086 - mae: 8.2244 - mse: 104.6086 - val_loss: 100.6015 - val_mae: 8.1696 - val_mse: 100.6015\n",
      "Epoch 19/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.0583 - mae: 8.2836 - mse: 105.0583 - val_loss: 100.2655 - val_mae: 8.0899 - val_mse: 100.2655\n",
      "Epoch 20/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.0340 - mae: 8.2661 - mse: 105.0340 - val_loss: 100.4009 - val_mae: 8.1159 - val_mse: 100.4009\n",
      "Epoch 21/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.7072 - mae: 8.2557 - mse: 104.7072 - val_loss: 100.3962 - val_mae: 8.1361 - val_mse: 100.3962\n",
      "Epoch 22/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.7237 - mae: 8.2532 - mse: 104.7237 - val_loss: 100.4942 - val_mae: 8.0861 - val_mse: 100.4942\n",
      "Epoch 23/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 104.9008 - mae: 8.2707 - mse: 104.9008 - val_loss: 100.7717 - val_mae: 8.1051 - val_mse: 100.7717\n",
      "Epoch 24/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.8084 - mae: 8.2579 - mse: 104.8084 - val_loss: 100.4249 - val_mae: 8.1405 - val_mse: 100.4249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21053eddb20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good one\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('NewData.csv')\n",
    "\n",
    "# # Normalize features\n",
    "# data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Split into train and test\n",
    "X = data[['RMT %', 'AGE', 'Depression', 'Stress', 'Anxiety', 'Depression.1']]\n",
    "y = data['Depression.2']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))  \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "            \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "          epochs=200, callbacks= early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 320ms/step\n",
      "Predicted Depression: 10.68\n"
     ]
    }
   ],
   "source": [
    "x_in = [[0.32, 68, 38, 18, 8, 14]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Depression: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Depression_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 203ms/step\n",
      "Predicted Depression: 10.68\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Depression_2.h5')\n",
    "\n",
    "x_in = [[0.32, 68, 38, 18, 8, 14]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Depression: {pred[0][0]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "188/188 [==============================] - 6s 14ms/step - loss: 332.9291 - mae: 15.1110 - mse: 332.9291 - val_loss: 215.1393 - val_mae: 11.9934 - val_mse: 215.1393\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 137.8525 - mae: 9.6756 - mse: 137.8525 - val_loss: 116.7180 - val_mae: 8.9783 - val_mse: 116.7180\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 113.6247 - mae: 9.0298 - mse: 113.6247 - val_loss: 114.4419 - val_mae: 8.9999 - val_mse: 114.4419\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 112.8366 - mae: 8.9880 - mse: 112.8366 - val_loss: 113.0229 - val_mae: 8.9504 - val_mse: 113.0229\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 112.2925 - mae: 8.9788 - mse: 112.2925 - val_loss: 109.0808 - val_mae: 8.8592 - val_mse: 109.0808\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 111.4336 - mae: 8.9569 - mse: 111.4336 - val_loss: 109.1053 - val_mae: 8.8674 - val_mse: 109.1053\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 112.0555 - mae: 8.9856 - mse: 112.0555 - val_loss: 110.9229 - val_mae: 8.8864 - val_mse: 110.9229\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 111.1496 - mae: 8.9503 - mse: 111.1496 - val_loss: 107.7911 - val_mae: 8.8034 - val_mse: 107.7911\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 110.9292 - mae: 8.9299 - mse: 110.9292 - val_loss: 109.0193 - val_mae: 8.8498 - val_mse: 109.0193\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 111.3433 - mae: 8.9638 - mse: 111.3433 - val_loss: 112.0816 - val_mae: 8.9339 - val_mse: 112.0816\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 110.7410 - mae: 8.9256 - mse: 110.7410 - val_loss: 109.6869 - val_mae: 8.9052 - val_mse: 109.6869\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 111.0134 - mae: 8.9488 - mse: 111.0134 - val_loss: 108.4694 - val_mae: 8.8442 - val_mse: 108.4694\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 110.7821 - mae: 8.9462 - mse: 110.7821 - val_loss: 109.1585 - val_mae: 8.8627 - val_mse: 109.1585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21061d8b5b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good one\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('NewData.csv')\n",
    "\n",
    "# # Normalize features\n",
    "# data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Split into train and test\n",
    "X = data[['RMT %', 'AGE', 'Depression', 'Stress', 'Anxiety']]\n",
    "y = data['Stress.1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))  \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "            \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "          epochs=200, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 104ms/step\n",
      "Predicted Stress: 16.75\n"
     ]
    }
   ],
   "source": [
    "x_in = [[0.48, 38, 26, 4, 22]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Stress: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Stress_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 249ms/step\n",
      "Predicted Stress: 16.75\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Stress_1.h5')\n",
    "\n",
    "x_in = [[0.48, 38, 26, 4, 22]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Stress: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "188/188 [==============================] - 6s 14ms/step - loss: 174.0048 - mae: 10.4671 - mse: 174.0048 - val_loss: 99.3486 - val_mae: 8.0506 - val_mse: 99.3486\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 84.0828 - mae: 7.6436 - mse: 84.0828 - val_loss: 76.1010 - val_mae: 7.5070 - val_mse: 76.1010\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 76.0698 - mae: 7.5045 - mse: 76.0698 - val_loss: 73.3673 - val_mae: 7.4110 - val_mse: 73.3673\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 74.7298 - mae: 7.4509 - mse: 74.7298 - val_loss: 73.6994 - val_mae: 7.4572 - val_mse: 73.6994\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 74.8524 - mae: 7.4831 - mse: 74.8524 - val_loss: 74.6685 - val_mae: 7.4463 - val_mse: 74.6685\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 74.3491 - mae: 7.4704 - mse: 74.3491 - val_loss: 72.1097 - val_mae: 7.3609 - val_mse: 72.1097\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 73.7914 - mae: 7.4248 - mse: 73.7914 - val_loss: 73.5152 - val_mae: 7.3892 - val_mse: 73.5152\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 73.7101 - mae: 7.4382 - mse: 73.7101 - val_loss: 71.8857 - val_mae: 7.3616 - val_mse: 71.8857\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 73.6667 - mae: 7.4393 - mse: 73.6667 - val_loss: 71.7435 - val_mae: 7.3709 - val_mse: 71.7435\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 73.0757 - mae: 7.4251 - mse: 73.0757 - val_loss: 72.2123 - val_mae: 7.3856 - val_mse: 72.2123\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 73.6949 - mae: 7.4378 - mse: 73.6949 - val_loss: 72.1111 - val_mae: 7.3925 - val_mse: 72.1111\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 73.7164 - mae: 7.4416 - mse: 73.7164 - val_loss: 71.1578 - val_mae: 7.3613 - val_mse: 71.1578\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 73.3195 - mae: 7.4303 - mse: 73.3195 - val_loss: 72.1983 - val_mae: 7.3744 - val_mse: 72.1983\n",
      "Epoch 14/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 73.2527 - mae: 7.4151 - mse: 73.2527 - val_loss: 74.0702 - val_mae: 7.4011 - val_mse: 74.0702\n",
      "Epoch 15/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 73.4296 - mae: 7.4303 - mse: 73.4296 - val_loss: 71.3751 - val_mae: 7.3615 - val_mse: 71.3751\n",
      "Epoch 16/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 72.7520 - mae: 7.4071 - mse: 72.7520 - val_loss: 71.3595 - val_mae: 7.3538 - val_mse: 71.3595\n",
      "Epoch 17/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 72.3513 - mae: 7.3862 - mse: 72.3513 - val_loss: 71.1342 - val_mae: 7.3399 - val_mse: 71.1342\n",
      "Epoch 18/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 72.3038 - mae: 7.3677 - mse: 72.3038 - val_loss: 71.3757 - val_mae: 7.3497 - val_mse: 71.3757\n",
      "Epoch 19/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 73.2021 - mae: 7.4381 - mse: 73.2021 - val_loss: 71.8448 - val_mae: 7.3647 - val_mse: 71.8448\n",
      "Epoch 20/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 72.5192 - mae: 7.3868 - mse: 72.5192 - val_loss: 71.5053 - val_mae: 7.3491 - val_mse: 71.5053\n",
      "Epoch 21/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 72.5667 - mae: 7.3905 - mse: 72.5667 - val_loss: 72.0743 - val_mae: 7.3497 - val_mse: 72.0743\n",
      "Epoch 22/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 72.8694 - mae: 7.4069 - mse: 72.8694 - val_loss: 71.2800 - val_mae: 7.3415 - val_mse: 71.2800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21061fd4df0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good one\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('NewData.csv')\n",
    "\n",
    "# # Normalize features\n",
    "# data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Split into train and test\n",
    "X = data[['RMT %', 'AGE', 'Depression', 'Stress', 'Anxiety', 'Stress.1']]\n",
    "y = data['Stress.2']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))  \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "            \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "          epochs=200, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 252ms/step\n",
      "Predicted Stress: 11.57\n"
     ]
    }
   ],
   "source": [
    "x_in = [[0.48, 38, 26, 4, 22, 16]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Stress: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Stress_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 201ms/step\n",
      "Predicted Stress: 11.57\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Stress_2.h5')\n",
    "\n",
    "x_in = [[0.48, 38, 26, 4, 22, 16]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Stress: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "188/188 [==============================] - 6s 15ms/step - loss: 194.7466 - mae: 11.1696 - mse: 194.7466 - val_loss: 114.0803 - val_mae: 8.5795 - val_mse: 114.0803\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 98.7124 - mae: 8.2554 - mse: 98.7124 - val_loss: 90.9394 - val_mae: 7.9712 - val_mse: 90.9394\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 2s 11ms/step - loss: 89.6354 - mae: 8.0269 - mse: 89.6354 - val_loss: 90.2804 - val_mae: 7.9844 - val_mse: 90.2804\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 87.8252 - mae: 7.9625 - mse: 87.8252 - val_loss: 86.0233 - val_mae: 7.9211 - val_mse: 86.0233\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 88.1315 - mae: 7.9966 - mse: 88.1315 - val_loss: 84.7024 - val_mae: 7.8088 - val_mse: 84.7024\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 87.5611 - mae: 7.9893 - mse: 87.5611 - val_loss: 84.3815 - val_mae: 7.8400 - val_mse: 84.3815\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 87.0114 - mae: 7.9592 - mse: 87.0114 - val_loss: 85.8712 - val_mae: 7.8469 - val_mse: 85.8712\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 87.2179 - mae: 7.9423 - mse: 87.2179 - val_loss: 84.2176 - val_mae: 7.7957 - val_mse: 84.2176\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 87.0388 - mae: 7.9589 - mse: 87.0388 - val_loss: 84.1197 - val_mae: 7.8432 - val_mse: 84.1197\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 86.8759 - mae: 7.9593 - mse: 86.8759 - val_loss: 84.1805 - val_mae: 7.8690 - val_mse: 84.1805\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 86.4445 - mae: 7.9177 - mse: 86.4445 - val_loss: 84.1727 - val_mae: 7.7832 - val_mse: 84.1727\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 86.8628 - mae: 7.9231 - mse: 86.8628 - val_loss: 84.2939 - val_mae: 7.8064 - val_mse: 84.2939\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 86.6434 - mae: 7.9339 - mse: 86.6434 - val_loss: 84.2805 - val_mae: 7.8317 - val_mse: 84.2805\n",
      "Epoch 14/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 86.7729 - mae: 7.9506 - mse: 86.7729 - val_loss: 84.4820 - val_mae: 7.7967 - val_mse: 84.4820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2106540eee0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good one\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('NewData.csv')\n",
    "\n",
    "# # Normalize features\n",
    "# data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Split into train and test\n",
    "X = data[['RMT %', 'AGE', 'Depression', 'Stress', 'Anxiety']]\n",
    "y = data['Anxiety.1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))  \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "            \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "          epochs=200, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 98ms/step\n",
      "Predicted Anxiety: 10.63\n"
     ]
    }
   ],
   "source": [
    "x_in = [[0.43, 38, 26, 14, 12]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Anxiety: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Anxiety_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted Anxiety: 11.66\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Anxiety_1.h5')\n",
    "\n",
    "x_in = [[0.48, 38, 26, 4, 22]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Anxiety: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "188/188 [==============================] - 6s 16ms/step - loss: 105.6667 - mae: 7.6141 - mse: 105.6667 - val_loss: 69.4444 - val_mae: 6.1917 - val_mse: 69.4444\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 69.1897 - mae: 6.4372 - mse: 69.1897 - val_loss: 59.5649 - val_mae: 6.2604 - val_mse: 59.5649\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 65.0224 - mae: 6.4330 - mse: 65.0224 - val_loss: 59.2878 - val_mae: 6.1176 - val_mse: 59.2878\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 64.0574 - mae: 6.4139 - mse: 64.0574 - val_loss: 60.0134 - val_mae: 6.2652 - val_mse: 60.0134\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 64.0842 - mae: 6.4045 - mse: 64.0842 - val_loss: 58.3477 - val_mae: 6.1387 - val_mse: 58.3477\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 63.8019 - mae: 6.3963 - mse: 63.8019 - val_loss: 57.8113 - val_mae: 6.0489 - val_mse: 57.8113\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 63.8076 - mae: 6.3846 - mse: 63.8076 - val_loss: 58.3166 - val_mae: 6.0741 - val_mse: 58.3166\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 63.5079 - mae: 6.3993 - mse: 63.5079 - val_loss: 57.7154 - val_mae: 6.0805 - val_mse: 57.7154\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 63.5573 - mae: 6.3745 - mse: 63.5573 - val_loss: 58.3591 - val_mae: 6.1492 - val_mse: 58.3591\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 63.1546 - mae: 6.3556 - mse: 63.1546 - val_loss: 58.4500 - val_mae: 6.1421 - val_mse: 58.4500\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 63.2736 - mae: 6.3817 - mse: 63.2736 - val_loss: 58.3981 - val_mae: 6.0977 - val_mse: 58.3981\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 63.2315 - mae: 6.3515 - mse: 63.2315 - val_loss: 58.1461 - val_mae: 6.0784 - val_mse: 58.1461\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 63.4070 - mae: 6.3939 - mse: 63.4070 - val_loss: 58.2769 - val_mae: 6.0964 - val_mse: 58.2769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x210677555e0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good one\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('NewData.csv')\n",
    "\n",
    "# # Normalize features\n",
    "# data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Split into train and test\n",
    "X = data[['RMT %', 'AGE', 'Depression', 'Stress', 'Anxiety', 'Anxiety.1']]\n",
    "y = data['Anxiety.2']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))  \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "            \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "          epochs=200, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step\n",
      "Predicted Anxiety: 11.07\n"
     ]
    }
   ],
   "source": [
    "x_in = [[0.48, 38, 26, 4, 22, 12]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Anxiety: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Anxiety_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 296ms/step\n",
      "Predicted Anxiety: 11.07\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Anxiety_2.h5')\n",
    "\n",
    "x_in = [[0.48, 38, 26, 4, 22, 12]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Anxiety: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "188/188 [==============================] - 4s 10ms/step - loss: 107.9193 - mae: 8.9700 - mse: 107.9193 - val_loss: 54.9456 - val_mae: 5.7707 - val_mse: 54.9456\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 28.2839 - mae: 4.1556 - mse: 28.2839 - val_loss: 20.7566 - val_mae: 3.8248 - val_mse: 20.7566\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 24.7899 - mae: 4.0928 - mse: 24.7899 - val_loss: 21.2434 - val_mae: 3.8757 - val_mse: 21.2434\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 23.8873 - mae: 4.0101 - mse: 23.8873 - val_loss: 21.2243 - val_mae: 3.7706 - val_mse: 21.2243\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 23.5693 - mae: 4.0011 - mse: 23.5693 - val_loss: 24.0188 - val_mae: 4.1138 - val_mse: 24.0188\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 23.1074 - mae: 3.9868 - mse: 23.1074 - val_loss: 21.4568 - val_mae: 3.9538 - val_mse: 21.4568\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 23.0867 - mae: 3.9734 - mse: 23.0867 - val_loss: 20.6268 - val_mae: 3.8889 - val_mse: 20.6268\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 22.8490 - mae: 3.9798 - mse: 22.8490 - val_loss: 20.5996 - val_mae: 3.8377 - val_mse: 20.5996\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 22.7875 - mae: 3.9653 - mse: 22.7875 - val_loss: 20.6506 - val_mae: 3.8857 - val_mse: 20.6506\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 22.6060 - mae: 3.9586 - mse: 22.6060 - val_loss: 21.1762 - val_mae: 3.9029 - val_mse: 21.1762\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 22.4472 - mae: 3.9385 - mse: 22.4472 - val_loss: 20.7739 - val_mae: 3.9382 - val_mse: 20.7739\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 22.3598 - mae: 3.9462 - mse: 22.3598 - val_loss: 20.4583 - val_mae: 3.7850 - val_mse: 20.4583\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 22.3642 - mae: 3.9374 - mse: 22.3642 - val_loss: 20.4940 - val_mae: 3.8125 - val_mse: 20.4940\n",
      "Epoch 14/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 22.2084 - mae: 3.9229 - mse: 22.2084 - val_loss: 20.3175 - val_mae: 3.8293 - val_mse: 20.3175\n",
      "Epoch 15/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 22.0069 - mae: 3.9185 - mse: 22.0069 - val_loss: 20.4663 - val_mae: 3.8269 - val_mse: 20.4663\n",
      "Epoch 16/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 22.0516 - mae: 3.9270 - mse: 22.0516 - val_loss: 20.4848 - val_mae: 3.8675 - val_mse: 20.4848\n",
      "Epoch 17/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 21.8993 - mae: 3.9077 - mse: 21.8993 - val_loss: 20.6911 - val_mae: 3.8826 - val_mse: 20.6911\n",
      "Epoch 18/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 21.7783 - mae: 3.9109 - mse: 21.7783 - val_loss: 20.3823 - val_mae: 3.8316 - val_mse: 20.3823\n",
      "Epoch 19/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 21.8135 - mae: 3.9094 - mse: 21.8135 - val_loss: 20.5708 - val_mae: 3.8029 - val_mse: 20.5708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21060a60640>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good one\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('NewData.csv')\n",
    "\n",
    "# # Normalize features\n",
    "# data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Split into train and test\n",
    "X = data[['RMT %', 'AGE', 'Depression', 'Stress', 'Anxiety', 'Depression.1', 'Stress.1', 'Anxiety.1' ]]\n",
    "y = data['Total Score.1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))  \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "            \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "          epochs=200, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 170ms/step\n",
      "Predicted Anxiety: 11.25\n"
     ]
    }
   ],
   "source": [
    "x_in = [[0.48, 38, 26, 22, 4, 14, 8, 2]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Anxiety: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Total_Score_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 127ms/step\n",
      "Predicted Anxiety: 11.25\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Total_Score_1.h5')\n",
    "\n",
    "x_in = [[0.48, 38, 26, 22, 4, 14, 8, 2]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Anxiety: {pred[0][0]:.2f}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
